{
  "dashboard": {
    "id": null,
    "title": "ChatQnA AI/LLM Metrics (TGI + vLLM)",
    "tags": ["chatqna", "ai", "llm", "tgi", "vllm", "metrics"],
    "timezone": "browser",
    "schemaVersion": 36,
    "version": 1,
    "refresh": "10s",
    "panels": [
      {
        "type": "row",
        "title": "TGI (Text Generation Inference) Metrics",
        "gridPos": {"h": 1, "w": 24, "x": 0, "y": 0},
        "collapsed": false
      },
      {
        "type": "timeseries",
        "title": "TGI Request Rate",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(tgi_request_count[1m])",
          "legendFormat": "Requests/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 1}
      },
      {
        "type": "timeseries",
        "title": "TGI Success Rate",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(tgi_request_success[1m])",
          "legendFormat": "Success/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 8, "y": 1}
      },
      {
        "type": "timeseries",
        "title": "TGI Error Rate",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(tgi_request_count[1m]) - rate(tgi_request_success[1m])",
          "legendFormat": "Errors/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 16, "y": 1}
      },
      {
        "type": "timeseries",
        "title": "TGI Prefill Operations",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(tgi_batch_inference_count{method=\"prefill\"}[1m])",
          "legendFormat": "Prefill/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 7}
      },
      {
        "type": "timeseries",
        "title": "TGI Decode Operations",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(tgi_batch_inference_count{method=\"decode\"}[1m])",
          "legendFormat": "Decode/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 8, "y": 7}
      },
      {
        "type": "timeseries",
        "title": "TGI Batch Concat (Backpressure)",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(tgi_batch_concat{reason=\"backpressure\"}[1m])",
          "legendFormat": "Backpressure Events/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 16, "y": 7}
      },
      {
        "type": "row",
        "title": "vLLM Metrics",
        "gridPos": {"h": 1, "w": 24, "x": 0, "y": 13},
        "collapsed": false
      },
      {
        "type": "timeseries",
        "title": "vLLM Request Rate",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(vllm_requests_total[1m])",
          "legendFormat": "vLLM Requests/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 14}
      },
      {
        "type": "timeseries",
        "title": "vLLM Success Rate",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(vllm_requests_success_total[1m])",
          "legendFormat": "vLLM Success/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 8, "y": 14}
      },
      {
        "type": "timeseries",
        "title": "vLLM Error Rate",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(vllm_requests_total[1m]) - rate(vllm_requests_success_total[1m])",
          "legendFormat": "vLLM Errors/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 16, "y": 14}
      },
      {
        "type": "timeseries",
        "title": "vLLM Token Generation Rate",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(vllm_tokens_generated_total[1m])",
          "legendFormat": "vLLM Tokens/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 20}
      },
      {
        "type": "timeseries",
        "title": "vLLM Response Time (95th percentile)",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "histogram_quantile(0.95, rate(vllm_request_duration_seconds_bucket[5m]))",
          "legendFormat": "vLLM 95th %"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 8, "y": 20}
      },
      {
        "type": "timeseries",
        "title": "vLLM Queue Size",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "vllm_queue_size",
          "legendFormat": "vLLM Queue"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 16, "y": 20}
      },
      {
        "type": "row",
        "title": "TEI (Text Embeddings Inference) Metrics",
        "gridPos": {"h": 1, "w": 24, "x": 0, "y": 26},
        "collapsed": false
      },
      {
        "type": "timeseries",
        "title": "TEI Embedding Request Rate",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(te_request_count{method=\"single\"}[1m])",
          "legendFormat": "Embedding Requests/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 27}
      },
      {
        "type": "timeseries",
        "title": "TEI Embedding Success Rate",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(te_embed_success[1m])",
          "legendFormat": "Embedding Success/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 8, "y": 27}
      },
      {
        "type": "timeseries",
        "title": "TEI Embedding Duration (95th percentile)",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "histogram_quantile(0.95, rate(te_embed_duration_bucket[5m]))",
          "legendFormat": "95th percentile"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 16, "y": 27}
      },
      {
        "type": "timeseries",
        "title": "TEI Queue Size",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "te_queue_size",
          "legendFormat": "Queue Size"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 33}
      },
      {
        "type": "row",
        "title": "Retrieval & Reranking Metrics",
        "gridPos": {"h": 1, "w": 24, "x": 0, "y": 39},
        "collapsed": false
      },
      {
        "type": "timeseries",
        "title": "Retriever Request Rate (TGI)",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(http_requests_total{job=\"retriever-tgi\"}[1m])",
          "legendFormat": "Retriever TGI Requests/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 40}
      },
      {
        "type": "timeseries",
        "title": "Retriever Request Rate (vLLM)",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(http_requests_total{job=\"retriever-vllm\"}[1m])",
          "legendFormat": "Retriever vLLM Requests/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 8, "y": 40}
      },
      {
        "type": "timeseries",
        "title": "Retriever Response Time",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=\"retriever-tgi\"}[5m]))",
            "legendFormat": "Retriever TGI 95th %"
          },
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=\"retriever-vllm\"}[5m]))",
            "legendFormat": "Retriever vLLM 95th %"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 16, "y": 40}
      },
      {
        "type": "row",
        "title": "Resource Utilization",
        "gridPos": {"h": 1, "w": 24, "x": 0, "y": 46},
        "collapsed": false
      },
      {
        "type": "timeseries",
        "title": "TGI CPU Usage",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "sum(rate(container_cpu_usage_seconds_total{container_label_com_docker_compose_service=\"chatqna-tgi-service\"}[1m])) * 100",
          "legendFormat": "TGI CPU %"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 47}
      },
      {
        "type": "timeseries",
        "title": "vLLM CPU Usage",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "sum(rate(container_cpu_usage_seconds_total{container_label_com_docker_compose_service=\"chatqna-vllm-service\"}[1m])) * 100",
          "legendFormat": "vLLM CPU %"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 8, "y": 47}
      },
      {
        "type": "timeseries",
        "title": "TEI CPU Usage",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "sum(rate(container_cpu_usage_seconds_total{container_label_com_docker_compose_service=\"chatqna-tei-embedding-service\"}[1m])) * 100",
            "legendFormat": "TEI Embedding CPU %"
          },
          {
            "expr": "sum(rate(container_cpu_usage_seconds_total{container_label_com_docker_compose_service=\"chatqna-tei-reranking-service\"}[1m])) * 100",
            "legendFormat": "TEI Reranking CPU %"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 16, "y": 47}
      },
      {
        "type": "timeseries",
        "title": "TGI Memory Usage",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "container_memory_usage_bytes{container_label_com_docker_compose_service=\"chatqna-tgi-service\"} / 1024 / 1024",
          "legendFormat": "TGI Memory MB"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 53}
      },
      {
        "type": "timeseries",
        "title": "vLLM Memory Usage",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "container_memory_usage_bytes{container_label_com_docker_compose_service=\"chatqna-vllm-service\"} / 1024 / 1024",
          "legendFormat": "vLLM Memory MB"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 8, "y": 53}
      },
      {
        "type": "timeseries",
        "title": "TEI Memory Usage",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "container_memory_usage_bytes{container_label_com_docker_compose_service=\"chatqna-tei-embedding-service\"} / 1024 / 1024",
            "legendFormat": "TEI Embedding Memory MB"
          },
          {
            "expr": "container_memory_usage_bytes{container_label_com_docker_compose_service=\"chatqna-tei-reranking-service\"} / 1024 / 1024",
            "legendFormat": "TEI Reranking Memory MB"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 16, "y": 53}
      },
      {
        "type": "row",
        "title": "GPU Utilization",
        "gridPos": {"h": 1, "w": 24, "x": 0, "y": 59},
        "collapsed": false
      },
      {
        "type": "timeseries",
        "title": "GPU Utilization",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "nvidia_gpu_utilization",
            "legendFormat": "GPU Utilization %"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 60}
      },
      {
        "type": "timeseries",
        "title": "GPU Memory Usage",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "nvidia_gpu_memory_used_bytes / 1024 / 1024 / 1024",
            "legendFormat": "GPU Memory GB"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 8, "y": 60}
      },
      {
        "type": "timeseries",
        "title": "GPU Temperature",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "nvidia_gpu_temperature_celsius",
            "legendFormat": "GPU Temperature °C"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 16, "y": 60}
      },
      {
        "type": "row",
        "title": "Model Performance",
        "gridPos": {"h": 1, "w": 24, "x": 0, "y": 66},
        "collapsed": false
      },
      {
        "type": "timeseries",
        "title": "TGI Token Generation Rate",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(tgi_generated_tokens[1m])",
          "legendFormat": "TGI Tokens/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 67}
      },
      {
        "type": "timeseries",
        "title": "vLLM Token Generation Rate",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "rate(vllm_tokens_generated_total[1m])",
          "legendFormat": "vLLM Tokens/s"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 8, "y": 67}
      },
      {
        "type": "timeseries",
        "title": "Average Response Time",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(tgi_request_duration_seconds_bucket[5m]))",
            "legendFormat": "TGI 95th %"
          },
          {
            "expr": "histogram_quantile(0.95, rate(vllm_request_duration_seconds_bucket[5m]))",
            "legendFormat": "vLLM 95th %"
          }
        ],
        "gridPos": {"h": 6, "w": 8, "x": 16, "y": 67}
      },
      {
        "type": "timeseries",
        "title": "TEI Average Embedding Time",
        "datasource": "Prometheus",
        "targets": [{
          "expr": "histogram_quantile(0.50, rate(te_embed_duration_bucket[5m]))",
          "legendFormat": "50th percentile"
        }],
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 73}
      },
      {
        "type": "row",
        "title": "System Health",
        "gridPos": {"h": 1, "w": 24, "x": 0, "y": 79},
        "collapsed": false
      },
      {
        "type": "stat",
        "title": "LLM Service Status",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "up{job=\"tgi-service\"}",
            "legendFormat": "TGI Service"
          },
          {
            "expr": "up{job=\"vllm-service\"}",
            "legendFormat": "vLLM Service"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "red", "value": null},
                {"color": "green", "value": 1}
              ]
            }
          }
        },
        "gridPos": {"h": 4, "w": 6, "x": 0, "y": 80}
      },
      {
        "type": "stat",
        "title": "TEI Service Status",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "up{job=\"tei-embedding\"}",
            "legendFormat": "TEI Embedding"
          },
          {
            "expr": "up{job=\"tei-reranking\"}",
            "legendFormat": "TEI Reranking"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "red", "value": null},
                {"color": "green", "value": 1}
              ]
            }
          }
        },
        "gridPos": {"h": 4, "w": 6, "x": 6, "y": 80}
      },
      {
        "type": "stat",
        "title": "Retriever Service Status",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "up{job=\"retriever-tgi\"}",
            "legendFormat": "Retriever TGI"
          },
          {
            "expr": "up{job=\"retriever-vllm\"}",
            "legendFormat": "Retriever vLLM"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "red", "value": null},
                {"color": "green", "value": 1}
              ]
            }
          }
        },
        "gridPos": {"h": 4, "w": 6, "x": 12, "y": 80}
      },
      {
        "type": "stat",
        "title": "API Gateway Status",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "up{job=\"chatqna-backend-tgi\"}",
            "legendFormat": "Backend TGI"
          },
          {
            "expr": "up{job=\"chatqna-backend-vllm\"}",
            "legendFormat": "Backend vLLM"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "red", "value": null},
                {"color": "green", "value": 1}
              ]
            }
          }
        },
        "gridPos": {"h": 4, "w": 6, "x": 18, "y": 80}
      },
      {
        "type": "row",
        "title": "Success Rates",
        "gridPos": {"h": 1, "w": 24, "x": 0, "y": 84},
        "collapsed": false
      },
      {
        "type": "stat",
        "title": "Request Success Rates",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "rate(tgi_request_success[5m]) / rate(tgi_request_count[5m]) * 100",
            "legendFormat": "TGI Success %"
          },
          {
            "expr": "rate(vllm_requests_success_total[5m]) / rate(vllm_requests_total[5m]) * 100",
            "legendFormat": "vLLM Success %"
          },
          {
            "expr": "rate(te_embed_success[5m]) / rate(te_request_count{method=\"single\"}[5m]) * 100",
            "legendFormat": "TEI Success %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "red", "value": null},
                {"color": "yellow", "value": 95},
                {"color": "green", "value": 99}
              ]
            }
          }
        },
        "gridPos": {"h": 4, "w": 12, "x": 0, "y": 85}
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "timepicker": {},
    "tooltip": {
      "shared": true,
      "sort": 0,
      "value_type": "individual"
    }
  },
  "folderId": 0,
  "overwrite": true
} 